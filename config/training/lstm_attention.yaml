# config/training/lstm_attention.yaml
# Training parameters optimized for LSTM+attention model

epochs: 200
batch_size: 48
learning_rate: 0.0001
weight_decay: 0.000005
gradient_clip_val: 1.0
early_stopping_patience: 10

# Sequence parameters optimized for attention
sequence_length: 16
overlap: 8

# Enhanced regularization for better generalization
use_class_weights: true
use_mixup: false  #

# Learning rate scheduling
use_cosine_scheduler: true
eta_min: 1e-7
warmup_epochs: 15
warmup_factor: 0.1